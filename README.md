# PromptInject

TODO: logo goes here.

Abstract:

> Transformer-based large language models (LLMs) provide a powerful foundation for natural language tasks in large-scale customer-facing applications. However, studies that explore their vulnerabilities emerging from malicious user interaction are scarce. By proposing PROMPTINJECT, a prosaic alignment framework for mask-based iterative adversarial prompt composition, we examine how GPT-3, the most widely deployed language model in production, can be easily misaligned by simple handcrafted inputs. In particular, we investigate two types of attacks -- goal hijacking and prompt leaking -- and demonstrate that even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit GPT-3â€™s stochastic nature, creating long-tail risks.

TODO: add framework figures (Figures 1 and 2 from the paper).

## Install

Run:

    pip install https://github.com/agencyenterprise/PromptInject

## Usage

See [notebooks/Example.ipynb](notebooks/Example.ipynb) for an example.

TODO: add more usage examples here.

## Cite

TODO

## Contributing

We appreciate any additional request and/or contribution to `PromptInject`. The [issues](/issues) tracker is used to keep a list of features and bugs to be worked on. Please see our [contributing documentation](/CONTRIBUTING.md) for some tips on getting started.
